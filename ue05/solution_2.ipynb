{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Consider the following setting: The input space is $\\mathcal{X} = [0, 1]$ and the output space is $\\mathcal{Y} = \\{\\pm 1\\}$. The input data $x \\in \\mathcal{X}$ is uniformly distributed. The Bayes classifier\n",
    "\n",
    "$$\n",
    "f^*(x)=\\begin{cases}\n",
    "+1 & x \\geq 0.3\\\\\n",
    "-1 & x < 0.3\n",
    "\\end{cases}.\n",
    "$$\n",
    "\n",
    "has zero Bayes risk $R^*=R(f^*) = 0$. The hypothesis class \n",
    "\n",
    "$$\n",
    "\\mathcal{H} = \\big\\{f_{+}, f_{-}\\big\\}\n",
    "$$\n",
    "\n",
    "consists of two classifiers $f_+(x) = +1$ and $f_-(x)=-1$ for all $x\\in \\mathcal{X}$. \n",
    "\n",
    "Is the Empirical Risk Minimization principle consistent with respect to $\\mathcal{H}$ and the uniform distribution over $\\mathcal{X}$ in the sense of lecture 1, slide 35?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Solution:**\n",
    "\n",
    "Yes. The hypothesis class $\\mathcal{H}$ is finite. Hence, $\\mathcal{H}$ is uniformly convergent. From the Theorem of Vapnik & Chervonenkis (lecture 4, slide 41) follows that ERM is universally consistent w.r.t. $\\mathcal{H}$. Hence, ERM is also consistent w.r.t. $\\mathcal{H}$ for the specific uniform distribution on $\\mathcal{X}$. \n",
    "\n",
    "**Note:** For the proof, it is sufficient to know that $\\mathcal{H}$ is finite. The answer is independent of the particular form of the classification problem, the joint distribution, and the hypothesis class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
