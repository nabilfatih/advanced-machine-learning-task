{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{X}$ be a space endowed with a metric $\\delta:\\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$. What is the VC dimension of the 1-nearest-neighbor classifier in $(\\mathcal{X}, \\delta)$? Discuss the implications of your result. \n",
    "\n",
    "*Note:* You can think of the metric space $(\\mathcal{X}, \\delta)$ as the Euclidean space $(\\mathbb{R}^d, \\|\\cdot\\|_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:** The VC dimension of 1NN is infinite. \n",
    "\n",
    "For every $n$, consider a set $\\mathcal{X}_n = \\{x_1, \\ldots, x_n\\}$ of $n$ distinct points. For an arbitrary labeling $y_1, \\ldots, y_n$, let $y:\\mathcal{X}_n \\to \\{\\pm 1\\}$ be the function that assigns each point $x_i \\in \\mathcal{X}_n$ to its class $y_i = y(x_i)$. In addition, the function \n",
    "\n",
    "$$\n",
    "\\text{nn}(x) = \\argmin_{x_i \\in \\mathcal{X}_n} \\delta(x_i, x)\n",
    "$$\n",
    "\n",
    "returns the nearest neighbor of $x$ in $\\mathcal{X}_n$. Then, 1NN assigns each point $x$ to the class $f_n(x) = y(\\text{nn}(x))$.\n",
    "\n",
    "Specifically, 1NN assigns each $x_i \\in \\mathcal{X}_n$ to \n",
    "\n",
    "$$\n",
    "f_n(x_i) = y(\\text{nn}(x_i)) = y(x_i) = y_i.\n",
    "$$\n",
    "\n",
    "As the labeling was arbitrary, the 1NN classifier can shatter $\\mathcal{X}_n$ for every $n\\in \\mathbb{N}$. This shows that the 1NN classifier has infinite VC dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** An infinite VC dimension implies that learning with 1NN is not possible: The 1NN classifier is not universally consistent. An infinite VC dimension implies that the hypothesis class implemented by 1NN is not uniformly convergent. From the Theorem of Vapnik and Chervonenkis follows that ERM with respect to the hypothesis class of 1NN is not universally consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Consider a uniform distribution over the input space $\\mathcal{X} = [0, 1]$. Suppose that the Bayes classifier is $f^*(x) = 1$ with Bayes risk $R^* = 0.1$. Thus, the labels are noisy with $P(y = 1 \\mid x) = 0.9$ for all $x \\in\\mathcal{X}$.\n",
    "\n",
    "The hypothesis class of the 1NN classifier contains the Bayes classifier $f^*$. To see this, choose a training set $\\mathcal{S} = \\big((x, 1)\\big)$ consisting of a singleton. Then the 1NN rule assigns each point of $\\mathcal{X}$ to class $1$.\n",
    "\n",
    "Let us ramdomly sample a training set \n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\big((x_1, y_1), \\ldots, (x_n, y_n)\\big).\n",
    "$$\n",
    "\n",
    "We expect that $10\\%$ of all $n$ points will have label $-1$, all other have label $+1$. The true risk of the 1NN classifier $f_n$ is approximately\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "R(f_n) \n",
    "&= \\mathbb{P}(f_n(x)\\neq y) \\\\\n",
    "&= \\mathbb{P}(y = 1 \\mid f_n(x)=0)\\mathbb{P}(f_n(x)=0) +  \\mathbb{P}(y = 0 \\mid f_n(x)=1)\\mathbb{P}(f_n(x)=1) \\\\\n",
    "&\\approx 0.9 \\cdot 0.1 + 0.1 \\cdot 0.9 = 0.18\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "We use the approximation sign $\\approx$, because the probabilities associated with $f_n$ are unknown. In addition, in this specific setting, we can regard $y$ and $f_n(x)$ as independent.   \n",
    "\n",
    "\n",
    "We can see that the risk $R(f_n)$ is approximately $0.18$, independently of the sample size $n$, while the Bayes risk $R^*$ would be $0.1$. Thus, the 1NN classifier is not universally (Bayes) consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Theorem** (Stone, 1977). Let $f_n$ be the k-nearest neighbor classifier constructed on $\\mathcal{S}_n$. If $n \\to\\infty$ and $k \\to \\infty$ such that $k/n \\to 0$, then the k-nearest neighbor classification rule is universally Bayes-consistent.\n",
    "\n",
    "**Remark:** The k-nearest neighbor classification rule is universally Bayes-consistent, if we choose the parameter $k$ such that it grows “slowly” with $n$, for example $k \\approx \\log(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Simulation\n",
    "\n",
    "Fit 1NN on a few data points and estimate its true risk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R(f_n) = 0.16\n"
     ]
    }
   ],
   "source": [
    "# fit model on few examples\n",
    "n = 100\n",
    "X = np.random.uniform(0, 1, n).reshape(-1, 1)\n",
    "y = np.random.choice([1, 0], size=n, p=[0.9, 0.1])\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# estimate true error of model\n",
    "n = 10000\n",
    "X = np.random.uniform(0, 1, n).reshape(-1, 1)\n",
    "y = np.random.choice([1, 0], size=n, p=[0.9, 0.1])\n",
    "R_fn = 1. - clf.score(X, y)\n",
    "\n",
    "print(f\"R(f_n) = {R_fn:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
