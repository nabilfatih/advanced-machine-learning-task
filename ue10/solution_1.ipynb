{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Using the notation from the lecture, show that \n",
    "\n",
    "$$\n",
    "W_e = \\sum_{i=1}^n w_i^{(t+1)} 1_{h_t(x_i) \\neq y_i} = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "for all $t\\in \\{1, \\ldots, T-1\\}$. Discuss this result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Proof:** \n",
    "\n",
    "Only examples misclassified by $h_t$ contribute to the sum $W_e$. Let $\\mathcal{I}_m = \\{i \\;:\\; h_t(x_i)\\neq y_i\\}$ be the index set of examples misclassified by $h_t$.\n",
    "\n",
    "Let $i \\in \\mathcal{I}_m$. From $h_t(x_i) \\neq y_i$ follows $y_ih_t(x_i) = -1$. Then the update rule of the weights is\n",
    "\n",
    "$$\n",
    "w_i^{(t+1)} = \\frac{w_i^{(t)} \\cdot \\exp(- y_i \\alpha_t h_t(x_i))}{Z_t} = \\frac{w_i^{(t)} \\cdot \\exp(\\alpha_t)}{Z_t}.\n",
    "$$\n",
    "\n",
    "By definition of $\\varepsilon_t^*$, we obtain\n",
    "\n",
    "$$\n",
    "\\varepsilon_t^* = \\varepsilon_t(h_t) \n",
    "= \\sum_{i=1}^n w_i^{(t)} 1_{h_t(x_i) \\neq y_i}\n",
    "= \\sum_{i\\in \\mathcal{I}_m} w_i^{(t)}.\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "From\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac{1}{2}\\ln \\frac{1-\\varepsilon_t^*}{\\varepsilon_t^*}\n",
    "$$\n",
    "\n",
    "follows \n",
    "\n",
    "$$\n",
    "\\exp(\\alpha_t) = \\sqrt{\\frac{1-\\varepsilon_t^*}{\\varepsilon_t^*}}.\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "According to Lemma 9.4., we have \n",
    "\n",
    "$$\n",
    "Z_t = 2\\sqrt{\\varepsilon_t^*(1-\\varepsilon_t^*)}.\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "Combining Equations (1) - (3) yields\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "W_e \n",
    "&= \\sum_{i=1}^n w_i^{(t+1)} 1_{h_t(x_i) \\neq y_i} \\\\\n",
    "&= \\sum_{i\\in \\mathcal{I}_m} \\frac{w_i^{(t)} \\cdot \\exp(\\alpha_t)}{Z_t}\\\\\n",
    "&= \\frac{\\exp(\\alpha_t)}{Z_t} \\sum_{i\\in \\mathcal{I}_m} w_i^{(t)} \\\\\n",
    "&= \\frac{\\exp(\\alpha_t)}{Z_t} \\cdot \\varepsilon_t^* \\\\\n",
    "&= \\frac{\\varepsilon_t^*}{2\\sqrt{\\varepsilon_t^*(1-\\varepsilon_t^*)}}\\sqrt{\\frac{1-\\varepsilon_t^*}{\\varepsilon_t^*}} \\\\\n",
    "&= \\frac{1}{2}.\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Discussion\n",
    "\n",
    "$W_e$ is the sum over all weights $w_i^{(t+1)}$ of examples misclassified by $h_t$. From \n",
    "\n",
    "$$\n",
    "W_e = \\varepsilon_{t+1}(h_t) = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "follows that the sum\n",
    "\n",
    "$$\n",
    "W_c = \\sum_{i=1}^n w_i^{(t+1)} 1_{h_t(x_i) = y_i} = 1 - \\varepsilon_{t+1}(h_t) = \\frac{1}{2}\n",
    "$$ \n",
    "\n",
    "over all weights $w_i^{(t+1)}$ of examples corretly classified by $h_t$ is also $1/2$. \n",
    "\n",
    "We have argued that AdaBoost updates the weights such that it \"forces\" the next base learner to focus on the problematic examples. This is achieved by increasing (decreasing) the weights of the misclassified (correctly classified) examples. The result of the exercise shows that the total weight is equally distributed over the misclassified and correctly classified examples. Since the number of misclassified (correctly classified) examples should decrease (increase) over the number of rounds, few misclassified examples should receive higher weights than many correctly classified examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
